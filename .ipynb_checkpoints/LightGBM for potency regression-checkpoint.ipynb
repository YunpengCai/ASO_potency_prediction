{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the csv file with selected motif and binding energy with determined potency\n",
    "df = pd.read_csv('data/...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate other target gapmers\n",
    "#df1 = pd.read_csv('data/...')\n",
    "\n",
    "#df=pd.concat([df,df1],axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pearson correlation\n",
    "\n",
    "corr_matrix = df.corr()\n",
    "print(corr_matrix[\"GC_content\"].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corr_matrix[\"binding_energy\"].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No need to perform PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create potency levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles=pd.DataFrame()\n",
    "quantiles['potency_level'] = pd.qcut(train[feature], 4,labels=['low','medium','high','very_high'])\n",
    "df=pd.concat([df, quantiles], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### One hot encoding for motif feature\n",
    "\n",
    "# use pd.concat to join the new columns with your original dataframe\n",
    "df = pd.concat([df,pd.get_dummies(df['motif'], prefix='motif')],axis=1)\n",
    "df.drop(['motif'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['potency'], axis=1)\n",
    "y = df['potency']\n",
    "X_train, X_valid, y_test, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test =sc.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lgb model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=list(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'regression',\n",
    "        'metric': {'l2', 'l1'},\n",
    "        'num_leaves': 40,\n",
    "        'learning_rate': 0.01,\n",
    "        'feature_fraction': 1,\n",
    "        'bagging_fraction': 0.9,\n",
    "        'bagging_freq': 5,\n",
    "        'verbose': 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "n_splits = 3 \n",
    "splits = list(KFold(n_splits=n_splits, shuffle=True).split(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof=np.zeros(len(X))\n",
    "for i, (train_idx, valid_idx) in enumerate(splits):  \n",
    "    print(f'Fold {i + 1}')\n",
    "    \n",
    "    lgd_train = lgb.Dataset(X[train_idx.astype(int)], label=y[train_idx.astype(int)])\n",
    "    lgd_valid = lgb.Dataset(X[valid_idx.astype(int)], label=y[valid_idx.astype(int)])\n",
    "    watchlist = [(lgb_train, 'train'), (val_data, 'valid')]\n",
    "    reg=lgb.train(params, lgd_train, 300, valid_sets = [trn_data, val_data], early_stopping_rounds=20, metrics='mae')\n",
    "    \n",
    "    oof[valid_idx] = reg.predict(X[valid_idx], num_iteration=reg.best_iteration)\n",
    "    \n",
    "    #predictions= reg.predict(X_test, num_iteration=reg.best_iteration)/n_splits \n",
    "    \n",
    "    #fold_importance_df = pd.DataFrame()\n",
    "    #fold_importance_df[\"feature\"] = features\n",
    "    #fold_importance_df[\"importance\"] = reg.feature_importance()\n",
    "    #fold_importance_df[\"fold\"] = i + 1\n",
    "    #feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "    \n",
    "    \n",
    "print (mean_absolute_error(y, oof))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN using Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define neural network with loss of MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorlayer as tl\n",
    "from tensorlayer.layers.core import Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "global_seed = 1111\n",
    "np.random.seed(global_seed)\n",
    "# Feature size is after one-hot encoding\n",
    "feature_size=12 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayerwithBN(tl.layers.Layer):\n",
    "    def __init__(\n",
    "            self,\n",
    "            prev_layer,\n",
    "            n_units=100,\n",
    "            is_train=False,\n",
    "            bn=False,\n",
    "            W_init=tf.truncated_normal_initializer(stddev=0.1, seed=global_seed),\n",
    "            b_init=tf.constant_initializer(value=0.0),\n",
    "            name='Dense_with_bn'\n",
    "    ):\n",
    "        Layer.__init__(self, prev_layer=prev_layer, name=name)\n",
    "        self.inputs = prev_layer.outputs\n",
    "        self.is_train = is_train\n",
    "\n",
    "        n_in = int(self.inputs.get_shape()[-1])  # obtain pre_layer's input number\n",
    "        with tf.variable_scope(name):\n",
    "            W = tf.get_variable(name='W', shape=(n_in, n_units), initializer=W_init, dtype=tf.float32)\n",
    "            b = tf.get_variable(name='b', shape=n_units, initializer=b_init, dtype=tf.float32)\n",
    "            w_x_b = tf.matmul(self.inputs, W) + b\n",
    "            if bn:\n",
    "                print(\"DenseLayer(bn)  %s: %d %s\" % (self.name, n_units, \"bn\"))\n",
    "                w_x_b = tf.layers.batch_normalization(w_x_b, training=self.is_train, name='norm')\n",
    "            else:\n",
    "                print(\"DenseLayer  %s: %d\" % (self.name, n_units))\n",
    "            self.outputs = tf.nn.relu(w_x_b)\n",
    "\n",
    "        # update layer paras\n",
    "        self.all_layers.append(self.outputs)\n",
    "        self.all_params.extend([W, b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNNRegressor:\n",
    "    \n",
    "    def __init__(self, sess, feature_size, hidden, bn=False, drop_out=False):\n",
    "        self.sess = sess\n",
    "        self.feature_size = feature_size\n",
    "        self.hidden = hidden\n",
    "        self.bn = bn\n",
    "        self.drop_out = drop_out\n",
    "        self.is_train = False\n",
    "        self.X_input = tf.placeholder(tf.float32, shape=[None, feature_size], name=\"X_input\")\n",
    "        self.y_input = tf.placeholder(tf.float32, shape=[None, 1], name=\"y_input\")\n",
    "\n",
    "        self.y_pred, self.paras, self.network = self.create_network()\n",
    "\n",
    "        self.loss = tl.cost.mean_squared_error(self.y_pred, self.y_input)\n",
    "\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(self.loss, var_list=self.paras)\n",
    "\n",
    "        tl.layers.initialize_global_variables(self.sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #hidden layers equals to list length of hidden units\n",
    "    def create_network(self):\n",
    "        input_x = self.X_input\n",
    "        network = tl.layers.InputLayer(inputs=input_x, name='input_layer')\n",
    "        ly = 0\n",
    "        for n_unit in self.hidden:\n",
    "            ly += 1\n",
    "            network = DenseLayerwithBN(network, n_units=n_unit,\n",
    "                                       is_train=self.is_train, bn=self.bn, name=\"Dense_bn\"+str(ly))\n",
    "            if self.drop_out:\n",
    "                network = tl.layers.DropoutLayer(network, keep=0.8, seed=global_seed, name='drop'+str(ly))\n",
    "        network = tl.layers.DenseLayer(network, n_units=1, act=tf.identity)\n",
    "\n",
    "        return network.outputs, network.all_params, network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, X):\n",
    "        \n",
    "        self.is_train = False\n",
    "        # For testing, disable dropout as follow.\n",
    "        feed_dict = {self.X_input: X}\n",
    "        dp_dict = tl.utils.dict_to_one(self.network.all_drop)\n",
    "        feed_dict.update(dp_dict)\n",
    "        y_pred = self.sess.run(self.y_pred, feed_dict=feed_dict)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def train(self, X, y, batch_size=32, n_epoch=2):\n",
    "        \"\"\"\n",
    "        :param X: train samples' X\n",
    "        :param y: train samples' label y\n",
    "        :param batch_size: mini batch size\n",
    "        :param n_epoch: training epoches\n",
    "        \"\"\"\n",
    "        self.is_train = True\n",
    "        step = 0\n",
    "        for epoch in range(n_epoch):\n",
    "            # print(\"****** train_epoch: %d \" % epoch)\n",
    "            batch_num = len(y)//batch_size+1\n",
    "            for i in range(batch_num):\n",
    "                batch_indices = np.random.randint(0, len(y), batch_size)\n",
    "                X_train_a = X[batch_indices]\n",
    "                y_train_a = y[batch_indices]\n",
    "                feed_dict = {self.X_input: X_train_a, self.y_input: y_train_a}\n",
    "                feed_dict.update(self.network.all_drop)\n",
    "                self.sess.run(self.optimizer, feed_dict=feed_dict)\n",
    "                step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the DNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=1111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sess = tf.InteractiveSession()\n",
    "dnn = DNNRegressor(sess=sess, feature_size=X.shape[-1], hidden=[32, 32, 32, 16, 16], bn=False, drop_out=True)\n",
    "\n",
    "# train the regressor\n",
    "dnn.train(X_train_ss, y_train_ss, batch_size=4, n_epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the regressor to predict test sets\n",
    "y_pred_ss = dnn.predict(X_test_ss)\n",
    "y_pred = ss_y.inverse_transform(y_pred_ss)\n",
    "\n",
    "\n",
    "MSE_test = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean squared error(test): %.4f\" % MSE_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
